---
title: "v1 Designing a Sampling Strategy for Mapping Accuracy Evaluation"
date: 2025-07-10T09:00:00Z
description: ""
author: "Vladimir Kroz & gpt o3-pro with search mode"
draft: true
tags: ["AI", "ML validation", "ML Quality", "Medical", "Data"]
---

## Problem statement

Evaluation of mapping quality requires an 'evaluation dataset', in which mappings are examined and annotated by human experts. Our challenge is that annotating data for evaluation dataset takes weeks of manual labor. The goal of this study is to determine sampling methods that produce evaluation datasets with minimal amount of records while maintaining statistical significance.
The primary goal of the sampling is twofold: (1) to estimate the overall mapping accuracy of the terms, and (2) to identify any systemic issues or patterns of errors.
Dataset Scale: Depending on domain, it ranges from 10 to 200 million terms.


---

Below is a self‑contained playbook for building a **minimal‑yet‑statistically‑sound sampling scheme** that lets you estimate mapping quality every 3‑4 months with ± 2 pp precision at 95 % confidence while still surfacing systemic errors.  The core idea is to start from the *mathematically* minimal simple‑random sample (\~2.4 k items), then cut that further by (i) variance‑efficient **stratified allocation** and (ii) **two‑phase / sequential** designs that stop early when evidence is already decisive.

## 1  How many records do you really need?

| Metric                                                   | Treated as…         | Conservative n (95 %, ±2 pp) | Notes                                                                                                         |
| -------------------------------------------------------- | ------------------- | ---------------------------- | ------------------------------------------------------------------------------------------------------------- |
| **Clinical Acceptability** (AS + AW vs R)                | Binomial proportion | 2 401                        | Z² p(1‑p)/E² with Z = 1.96, p = 0.5, E = 0.02 ([Wikipedia][1], [select-statistics.co.uk][2])                  |
| **Specificness** (AS vs AW, conditional on Acceptable)   | Binomial on sub‑set | \~1 600                      | If Acceptable≈0.8, effective N≈0.8 × total; keep total at ≥2 000 to keep ±2 pp on this conditional proportion |
| **Coverage** (high‑confidence predictions ÷ corpus size) | Known denominator   | Negligible                   | Requires no extra annotation—just count predictions marked “high confidence.”                                 |

*Take‑away*: **≈ 2 400 random records per cycle** is enough for all headline metrics with Wilson or Agresti‑Coull intervals, which are tighter than Wald yet still exact at small n ([Wikipedia][3]).

---

## 2  Why pure simple‑random sampling (SRS) is rarely optimal

* Error rates are almost never truly uniform; confidence bands, domain, or data‑source often explain large variance fragments in mapping tasks ([PMC][4], [ResearchGate][5]).
* SRS wastes reviews on high‑confidence items that almost never fail, while under‑sampling risky slices.
* You can slash required n by **20–40 %** with well‑chosen strata without losing unbiasedness, provided you weight back by inverse sampling probability.

---

## 3  Recommended sampling design

### 3.1 Define strata that matter

Stratify once, reuse forever.

| Stratum dimension      | Typical buckets                                  | Why it matters                                                                                                                              |
| ---------------------- | ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **Model confidence**   | Deciles or tertiles (e.g. 0–0.3, 0.3–0.7, 0.7–1) | Low confidence has higher variance; oversampling here reduces total variance efficiently ([homepages.ecs.vuw.ac.nz][6], [ScienceDirect][7]) |
| **Clinical domain**    | Labs vs Conditions vs Medications                | Different vocabularies (LOINC, SNOMED, RxNorm) show distinct error modes ([Oxford Academic][8])                                             |
| **Data source / site** | Hospital A, Hospital B, Out‑patient, etc.        | Local naming conventions influence mapping errors ([PMC][9])                                                                                |

Three dimensions with modest cardinalities (3 × 3 × 4 = 36 strata) usually suffice.

### 3.2 Allocate sample per stratum

Use **Neyman (optimal) allocation**:

n\_h = n · (N\_h σ\_h) / Σ(N\_k σ\_k) ([its.caltech.edu][10], [Wikipedia][11])

where N\_h is stratum size and σ\_h² an estimate of mapping‑accuracy variance (use prior rounds or pilot 300‑record scan if σ unknown).  High‑variance or large strata automatically receive more labels.

*Empirical rule*: after one pilot you can cut total n from 2 400 to **≈ 1 600** while keeping the same ±2 pp precision because variance collapses once low‑risk strata are correctly down‑weighted.

### 3.3 Oversampling for error discovery

Reserve **\~25 % of your budget** for an *“exploratory stratum”*: union of low confidence, new ontologies, recently retrained model components, etc.  Weight them back when computing corpus‑level metrics, but review every one to pick up emergent failure modes early.

---

## 4  Two‑phase & sequential tricks to save even more labels

| Technique                                    | How it works                                                                                                                                                                                                                    | When to use                                                                     | Gain                                                     |
| -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | -------------------------------------------------------- |
| **Two‑phase (double) sampling**              | Phase 1: pull 1 000 cheap machine‑scored records to estimate per‑stratum means/variances; Phase 2: draw human labels only where Phase 1 variance is high ([PennState: Statistics Online Courses][12], [dickbrus.github.io][13]) | You can derive proxy quality measures (e.g., string‑distance) at near‑zero cost | 10–30 % fewer expert labels                              |
| **Sequential Probability Ratio Test (SPRT)** | Evaluate items one‑by‑one; stop early if likelihood ratio crosses accept/reject thresholds ([Wikipedia][14], [SUNY Cortland - Services][15])                                                                                    | Quarterly “go/no‑go” acceptance of new model versions                           | Cuts average n by half when model is clearly good or bad |
| **Adaptive/active sampling**                 | Use model uncertainty to adapt sampling probabilities on the fly ([SpringerLink][16])                                                                                                                                           | Continuous deployment with human‑in‑the‑loop                                    | Finds rare systematic errors quickly                     |

---

## 5  Putting it all together: quarterly workflow

1. **Freeze sampling frame** (10–200 M terms) right after each model release.
2. **Compute stratum sizes & prior variances.**
3. **Draw Phase 1 proxy sample (optional)** to refine σ̂ if you use two‑phase.
4. **Allocate n via Neyman**; typical outcome after first cycle:

   * High‑confidence bucket: 300
   * Mid‑confidence: 500
   * Low‑confidence: 600
   * Exploratory slices: 400
     *Total ≈ 1 800 human labels*.
5. **Annotate with AS/AW/R labels.**
6. **Estimate metrics with inverse‑probability weighting**; compute Wilson CIs.
7. **Drift monitoring:** feed quarterly point estimates into an EWMA or CUSUM chart; set alert at ±3σ from historical mean.
8. **Root‑cause analysis:** mine the exploratory slice for error clusters; hand results to ontology or model owners.

---

## 6  Implementation checklist

| Component                 | Open‑source options                                         |
| ------------------------- | ----------------------------------------------------------- |
| Sampling frame build      | Spark/Presto SQL                                            |
| Stratified draw & weights | `pandas` plus `numpy.random.choice` or R’s `survey` package |
| Wilson / Agresti‑Coull CI | `statsmodels.stats.proportion`                              |
| Neyman allocation helper  | R `survey::stratify`, Python snippet in metrics pipeline    |
| Sequential tests          | `sequential` R package or custom Python (few lines)         |
| Dashboarding              | Superset / Metabase for CIs + control charts                |

---

## 7  Key take‑aways for your setting

* **Start simple**: run one cycle with **2 400 SRS items**—establish a variance baseline and confidence‑score correlation.
* From cycle 2 onward, **switch to stratified‑Neyman**; expect **≈ 1 600 labels** for ±2 pp overall accuracy.
* **Always over‑sample the riskiest 10–20 % confidence band** to keep error discovery strong; re‑weight in analysis.
* **Embed two‑phase or SPRT** if releases become more frequent or labeling budget tightens further.

These steps give you an auditable, statistically defensible evaluation set with the smallest practical human‑annotation footprint—freeing your experts to focus on fixing the real errors the sampling scheme will reliably uncover.

---

## Bibliography (most useful references)

* Sample‑size & margin‑of‑error calculators and theory ([Wikipedia][1], [select-statistics.co.uk][2], [SurveyMonkey][17])
* Wilson/Agresti‑Coull accuracy intervals ([Wikipedia][3])
* Stratified sampling fundamentals & Neyman allocation ([homepages.ecs.vuw.ac.nz][6], [ScienceDirect][7], [its.caltech.edu][10], [Wikipedia][11])
* Two‑phase sampling design ([PennState: Statistics Online Courses][12], [dickbrus.github.io][13])
* Sequential testing for acceptance sampling ([Wikipedia][14], [SUNY Cortland - Services][15])
* Adaptive sampling in ML contexts ([SpringerLink][16])
* Domain‑specific mapping error studies (LOINC) ([PMC][4], [ResearchGate][5])


[1]: https://en.wikipedia.org/wiki/Sample_size_determination?utm_source=chatgpt.com "Sample size determination"
[2]: https://select-statistics.co.uk/calculators/sample-size-calculator-population-proportion/?utm_source=chatgpt.com "Sample Size - Population Proportion - Select Statistical Consultants"
[3]: https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval?utm_source=chatgpt.com "Binomial proportion confidence interval - Wikipedia"
[4]: https://pmc.ncbi.nlm.nih.gov/articles/PMC3340474/?utm_source=chatgpt.com "An Approach to Improve LOINC Mapping through Augmentation of ..."
[5]: https://www.researchgate.net/publication/49967692_Correctness_of_Voluntary_LOINC_Mapping_for_Laboratory_Tests_in_Three_Large_Institutions?utm_source=chatgpt.com "(PDF) Correctness of Voluntary LOINC Mapping for Laboratory Tests ..."
[6]: https://homepages.ecs.vuw.ac.nz/~rarnold/STAT392/SampleSurveysBook/_book/stratified-sampling.html?utm_source=chatgpt.com "Chapter 8 Stratified Sampling | STAT392"
[7]: https://www.sciencedirect.com/topics/engineering/stratified-sampling?utm_source=chatgpt.com "Stratified Sampling - an overview | ScienceDirect Topics"
[8]: https://academic.oup.com/jamia/article/30/2/301/6809034?utm_source=chatgpt.com "Mis-mappings between a producer's quantitative test codes and ..."
[9]: https://pmc.ncbi.nlm.nih.gov/articles/PMC4832593/?utm_source=chatgpt.com "Learning From the Crowd in Terminology Mapping: The LOINC ..."
[10]: https://www.its.caltech.edu/~zuev/teaching/2013Spring/Math408-Lecture-20-21.pdf?utm_source=chatgpt.com "[PDF] Lecture 20-21. Neyman Allocation vs Proportional Allocation and ..."
[11]: https://en.wikipedia.org/wiki/Neyman_allocation?utm_source=chatgpt.com "Neyman allocation - Wikipedia"
[12]: https://online.stat.psu.edu/stat506/Lesson10.html?utm_source=chatgpt.com "10 Double or Two-Phase Sampling - STAT ONLINE"
[13]: https://dickbrus.github.io/SpatialSamplingwithR/Twophase.html?utm_source=chatgpt.com "Chapter 11 Two-phase random sampling | Spatial sampling with R"
[14]: https://en.wikipedia.org/wiki/Sequential_probability_ratio_test?utm_source=chatgpt.com "Sequential probability ratio test - Wikipedia"
[15]: https://web.cortland.edu/matresearch/BinSeqSTART.pdf?utm_source=chatgpt.com "[PDF] Understanding Binomial Sequential Testing"
[16]: https://link.springer.com/article/10.1007/s10922-024-09881-1?utm_source=chatgpt.com "Parametric Machine Learning-Based Adaptive Sampling Algorithm ..."
[17]: https://www.surveymonkey.com/mp/margin-of-error-calculator/?utm_source=chatgpt.com "Margin of error calculator (with formula and examples)"

---

## FAQ

1. Nature of the Mappings

Q: What type of mappings are you evaluating? (e.g., terminology mappings between medical vocabularies, entity resolution, language translations, data schema mappings)
A: Mapping between medical terms extracted from healthcare notes and medical records to numerical representation using codified systems like LOINC etc. 

Q: What constitutes a single "term" and what is it being mapped to?
A: Its usually sequaence of few words representing specific medical term. E.g. for lab results the term could be 'blood sugar', or 'glucose'. For conditions domain it could be 'cancer' or 'diabetes II'.

2. Evaluation Metrics & Error Types

Q: How do you currently measure mapping accuracy? (binary correct/incorrect, or more nuanced scoring?)
A: We label each model prediction with one of the values:
    - Acceptable Specific (AS) - when model prediction is highly accurate
    - Acceptable Wide (AW) - when model prediction is acceptable, but not the best
    - Rejected (R) - when model's prediction is not acceptable

  Based on these labels we compute evaluation metrics:

    - Accuracy (Clinical  Accetability): Accuracy = (AS+AW)/(AS+AW+R)

    - Specificness (Clinical Specificness): Specificness = AS/(AS+AW)

    - Coverage: metric is used to estimate how much of the entire corpus our model could map with high confidence. Coverage = "Number of high confidence predictions" /  "Total number of terms"

Q: What types of errors are most critical to detect?
A: Inaccurate mappings

Q: Are certain types of mappings more prone to errors than others?
A: Unknown

3. Current Process & Constraints

Q: How many records can your experts realistically annotate per day/week?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: Do you have a target sample size in mind based on your resources?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: What confidence level and margin of error would be acceptable for your accuracy estimates?
A: 95% confidence level and +/-2%error  

4. Dataset Characteristics

Q: Is the error rate expected to be uniform across the dataset, or are there subgroups with potentially different error rates?
A: Unknown. We start with hypothesis that errors are distributed uniformly, but we should provide technique to sample around specific terms groups if needed. 

Q: Do you have any prior knowledge about the expected overall accuracy rate?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: Are there metadata or features that could help stratify the population?
A: Yes, assume there are some metadata fields, such as data source, timestamps and few others. 

5. Statistical Requirements

Q: Do you need to detect rare but critical errors (requiring larger samples)?
A: No

Q: Is the goal to get a point estimate of accuracy, or also to compare accuracy across different subgroups?
A: Point estimate

Q: Do you need ongoing monitoring or is this a one-time evaluation?
A: Ongoing monitoring. We will be doing this every 3-4 months.
