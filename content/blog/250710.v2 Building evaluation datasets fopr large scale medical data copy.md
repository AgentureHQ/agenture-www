---
title: "v2 Designing a Sampling Strategy for Mapping Accuracy Evaluation"
date: 2025-07-10T09:00:00Z
description: ""
author: "Vladimir Kroz & gpt o3-pro"
draft: true
tags: ["AI", "ML validation", "ML Quality", "Medical", "Data"]
---


# Designing an optimal sampling for evaluation of mapping accuracy

## Problem statement

Evaluation of mapping quality requires an 'evaluation dataset', in which mappings are examined and annotated by human experts. Our challenge is that annotating data for evaluation dataset takes weeks of manual labor. The goal of this study is to determine sampling methods that produce evaluation datasets with minimal amount of records while maintaining statistical significance.
The primary goal of the sampling is twofold: (1) to estimate the overall mapping accuracy of the terms, and (2) to identify any systemic issues or patterns of errors.
Dataset Scale: Depending on domain, it ranges from 10 to 200 million terms.

## Objectives of the Evaluation Dataset

The evaluation dataset must serve two key purposes: **(1)** provide an accurate estimate of overall mapping accuracy (with statistical confidence), and **(2)** enable discovery of any **systematic error patterns** in the mapping model’s outputs. In our case, we are mapping medical terms (e.g. *“blood sugar”*, *“glucose”*, *“diabetes II”*) to standard codes (like LOINC) and labeling each prediction as Acceptable Specific (AS), Acceptable Wide (AW), or Rejected (R). The sampling strategy should therefore be designed to yield a **minimal yet representative** set of term mappings that achieves a desired statistical precision for accuracy metrics, while also including enough variety to spot recurring error trends.

## Sample Size Determination (Statistical Confidence)

To **estimate overall accuracy** with a 95% confidence level and a ±2% margin of error, we must calculate an appropriate sample size. Accuracy in this context can be treated as a proportion (the proportion of terms mapped acceptably). Using standard sample size formulas for proportions, the worst-case variance occurs at p = 0.5 (50% accuracy). Plugging in a 95% confidence level (Z = 1.96) and a 0.02 margin, we get:

$n = \left(\frac{1.96}{0.02}\right)^2 \times 0.25 \approx 2401.$

In other words, **around 2,400 samples** are required to achieve ±2% precision at 95% confidence. This is consistent with common guidelines (e.g. \~1,000 samples yields \~±3% margin, \~400 yields ±5%). By sampling \~2.4k term mappings and having experts annotate them (AS/AW/R), we can be 95% confident that the observed “Clinical Acceptability” (accuracy) is within 2 percentage points of the true value in the 10–200 million population. If we have prior reason to expect a very high accuracy (say 90%+), the required sample could be slightly smaller (because variance p(1–p) would be lower), but using the conservative worst-case ensures we **maintain statistical significance** even if accuracy is lower than expected.

*Note:* The **Coverage** metric (proportion of all terms the model maps with high confidence) can also be estimated from this sample if we sample terms irrespective of model confidence. However, if *Coverage* specifically refers to terms the model *flagged* as high-confidence, we may treat those as a sub-population for separate evaluation (discussed below).

## Sampling Methodology

Having determined the approximate sample size, the next step is deciding *how to select* those \~2,400 term mappings. The goal is to obtain a **representative sample of the term population** for unbiased accuracy estimates, while also capturing any diversity in the data that could affect error rates. We focus on **probability sampling** methods (each term has a known chance of selection) to allow valid inference about the whole population. Two complementary approaches are recommended:

### 1. Simple Random Sampling

A simple random sample means every term mapping in the full dataset has an equal chance of being chosen. This method is straightforward and **unbiased**, ensuring that our accuracy metrics (like acceptability and specificness) computed on the sample are **unbiased estimates** of true performance. Simple random sampling works well if the population of terms is homogeneous. In practice, this would involve randomly picking \~2,400 term instances out of the 10–200 million mappings.

**Pros:** It’s easy to implement and requires no prior grouping; if errors truly are uniformly distributed across all terms (our initial hypothesis), a pure random sample efficiently captures the overall error rate without over-complicating the design.

**Cons:** If the population is **heterogeneous** – for example, if certain subsets of terms (by data source, term type, etc.) have different accuracy – a purely random sample might, by chance, under-represent some important subgroups. This could obscure *systematic issues* that are confined to a subgroup. Also, if some categories are a small fraction of the total, a random sample might include very few or none of those, providing little insight into their specific error patterns.

### 2. Stratified Sampling

Given the potential diversity of terms, a **stratified random sampling** approach is highly recommended to complement the random sampling. Stratified sampling means we divide the population into meaningful **subgroups (strata)**, then sample randomly within each subgroup. This ensures each subgroup is represented in the evaluation according to its prevalence or importance. For our mapping evaluation, strata can be defined based on **metadata or inherent term characteristics**, for example:

* **Domain or Category** of term – e.g. laboratory tests vs. diagnoses vs. medications (if the mapping covers multiple domains). Each domain might have different mapping challenges.
* **Data source or origin** – if terms come from different hospitals, clinics, or note types, these could form strata (especially since metadata fields are available).
* **Term frequency or novelty** – one could stratify terms by frequency (common vs. rare terms). This addresses the possibility that very rare terms might have higher error rates.
* **Model confidence** – another approach is stratifying by the model’s own confidence score (e.g. high-confidence predictions vs. low-confidence). This is useful because one of our metrics, *Coverage*, focuses on high-confidence mappings; we may want to ensure we evaluate some low-confidence cases to see if they hide any systemic errors.

Using proportional stratified sampling, we would allocate the sample across strata **in proportion to their share of the population** to keep the overall sample representative. For example, if 30% of all term mappings are lab tests and 70% are diagnoses, the \~2400 sample might include roughly 720 lab test terms and 1680 diagnosis terms, sampled randomly within each category. This way, the overall accuracy estimate remains unbiased for the whole population, but we can also **compute accuracy within each stratum** and see if any group deviates.

The benefits of stratified sampling are well-documented: it tends to **increase precision of estimates** (lower variance) when there are real differences between strata. In fact, stratification can yield a smaller estimation error than simple random sampling, especially if the stratifying variable is related to accuracy outcomes. In our context, if (for example) mapping lab tests is generally easier (fewer errors) than mapping diagnoses, stratified sampling ensures we measure each separately and get an accurate overall mix. It also **highlights differences among groups** – allowing us to spot if one subgroup has a significantly lower acceptability rate, signaling a systematic issue.

**Stratified Sampling for Pattern Discovery:** Another advantage is flexibility – we can introduce **disproportionate stratified sampling** if needed to drill down into a subgroup. For instance, if a certain rare category of terms is suspected to have problems, we might oversample it (take more than its population proportion in the evaluation set) to get enough examples to assess its accuracy. This would be a conscious design choice to discover errors in that subgroup (we would weight or separate those samples when computing overall accuracy so as not to skew the aggregate). In general, *stratified random sampling provides a more nuanced and comprehensive picture of a heterogeneous population’s performance than a simple random sample*. It allows us to represent all important sub-populations of terms and thus is well-suited for finding **systemic error patterns** if they exist.

### Implementation Notes

* **Sampling Frame:** Ensure we have a proper list of all term mapping instances (the population) possibly with their metadata. If the population is extremely large (hundreds of millions), in practice we might randomly pre-select a manageable subset or use reservoir sampling techniques to draw random records without bias.
* **Random Selection Tools:** Use a random number generator or database sampling query to pick records. For stratified sampling, partition the data by strata first (e.g. by adding a stratifying key in queries).
* **Avoiding Bias:** Make sure the selection process does not inadvertently skip certain terms (for example, if data is sorted, a systematic sample could accidentally skip a pattern). Truly random or appropriately stratified-random selection will prevent **sampling bias**.
* **Sample Size per Stratum:** In proportional stratified sampling, some strata will have small sample counts if their population share is small. As a rule of thumb, try to have a minimum number in each important stratum (e.g. at least 30-50 samples even for tiny groups) to be able to observe any issues, adjusting slightly if needed. This may bump the total sample size up a bit (a minor increase to, say, 2500) – a trade-off for better subgroup insight.
* **Expert Annotation:** Once sampled, each term mapping will be manually annotated by experts with AS/AW/R labels. This is the time-consuming step, so our sampling must ensure every annotated record yields maximal information about the model’s performance.

## Identifying Systemic Error Patterns

With the evaluation dataset in hand (and annotated), we will compute overall **Clinical Acceptability (accuracy)** = (AS+AW) / total, **Specificness** = AS / (AS+AW), and Coverage as defined. The **second goal** is to discover patterns or recurring themes in the **Rejected (R)** cases that could indicate systematic mapping issues. Achieving this does not necessarily require a separate sampling method, but rather a careful analysis of the **error cases within the sample**. Here’s how the sampling design and follow-up analysis help reveal patterns:

* **Stratified results:** First, by examining accuracy and error rates *by stratum*, we can see if certain groups of terms have higher rejection rates. For example, if terms originating from Source A show 10% rejects while Source B shows 2%, that’s a clear signal of a systemic issue in Source A’s data or how the model handles it. Similarly, one domain (say lab tests) might have mostly AS/AW and another (say clinical notes diagnoses) has more R – indicating where to focus improvements. The stratified sample enables these **group-wise comparisons with statistical backing** (each subgroup’s sample provides an estimate of its own accuracy). If needed, formal statistical tests (chi-square, etc.) could be used to confirm differences, but often the patterns will be apparent if the disparity is large.

* **Qualitative error analysis:** Beyond statistics, we should **qualitatively analyze the misclassifications** (the R-labeled mappings in the sample). By reviewing these error cases, experts can look for commonalities: Are many of the errors involving similar terms or concepts? Do they fall into certain categories (e.g. a lot of abbreviation terms failing, or many errors with certain numeric values or units, etc.)? **Clustering the errors by theme** can identify patterns (for instance, perhaps all the lab test errors involve panels or all the diagnosis mapping errors involve severity modifiers). Since our sample is relatively small, experts can go through each error case and categorize the failure reasons. This addresses the second objective by pinpointing **systemic error types** (e.g. “the model confuses glucose tests that are fasting vs random” or “the model often rejects rare synonyms for common conditions”).

* **Random error sampling:** It can also be helpful to **zoom in on misclassifications specifically**. One technique is to take all the R cases from the evaluation sample and sample *among just those* to double-check or get more context (if the initial annotation didn’t capture enough detail on why it was wrong). According to best practices in ML error analysis, *“randomly selecting and analyzing misclassified samples can provide a deeper understanding of the reasons behind the errors and help identify common patterns.”* By examining these examples in depth, we might discover, for example, that a mapping error occurs whenever a term has a certain ambiguous word, or that the errors cluster around a particular subset of codes.

* **Feedback into sampling:** If the initial evaluation reveals a suspicious pattern (say all errors seem to come from a particular rare category), we should consider that in the *next* round of evaluation. For ongoing monitoring, it may be wise to **include a targeted stratum** for that category in future samples (even if disproportionate) to see if the issue persists or is getting fixed. The sampling method can thus evolve as we learn where the model struggles.

## Ongoing Monitoring Strategy

Since this evaluation will be conducted every 3–4 months, consistency in methodology is important for tracking performance over time. Here are recommendations for ongoing use of sampling:

1. **Fixed Sample Size & Confidence:** Continue using roughly the same sample size (≈2400) to maintain the ±2% margin of error on each evaluation. This allows comparison of overall accuracy between time points with known statistical certainty (differences larger than \~±2% will be significant changes).
2. **Consistent Stratification Schema:** Use the same stratification criteria for each round (e.g. if stratifying by domain and source, keep those definitions constant). This way, you can also compare subgroup accuracies over time (e.g. “Accuracy on lab terms improved from 95% to 97% after updates, while on diagnoses it stayed \~90%”). It also ensures no subgroup is accidentally dropped from evaluation in a given round.
3. **Random Fresh Samples:** Within each stratum, draw a new random sample each cycle. Do not re-use the exact same terms every time (to avoid overfitting the evaluation). The population of terms is huge, so there’s plenty of novel examples to draw. New random samples provide an independent assessment each time and avoid any bias from remember previously seen terms. \*(One caveat: if we specifically want to track **improvement on exact previously problematic terms**, we could retain a small *“sentinel” set of known difficult terms to test on each round in addition to the random sample. But those would be analyzed separately, not mixed into the random sample metrics.)*
4. **Analyze Trend and New Patterns:** After each evaluation, perform the error pattern analysis. Over time, some error patterns may disappear if fixed by model improvements, and new ones might emerge if the model is updated or if new types of data enter the system. The sampling approach (random + stratified) is flexible enough to detect these changes.
5. **Adjust Stratification if Needed:** As domains evolve or if certain strata consistently show issues, we might adjust our stratification. For example, if a new data source is added to the pipeline, we should include it as a stratum in the sample to ensure its quality is monitored. Or if initial hypothesis of uniform error distribution is wrong, we continue stratifying by the factors that matter (perhaps even increase sample size a bit in high-variance strata to pin down their accuracy with ±2%). The key is maintaining statistical rigor while focusing on areas of interest.

## Conclusion

In summary, to evaluate the mapping accuracy with minimal effort yet high confidence, we recommend using a **random sampling strategy of approximately 2,400 term mappings** drawn from the 10–200 million corpus, which yields a 95% confidence interval of about ±2% on accuracy. To meet both goals – overall accuracy estimation and error pattern identification – this random sample should be **stratified** across relevant categories of terms (such as domain, source, etc.) so that the evaluation dataset is **representative and comprehensive**. Stratified sampling will improve the precision of our estimates and ensure even small subgroups of terms are examined. After annotation, we will compute aggregate metrics (Accuracy, Specificness, Coverage) and also **drill down into errors**, leveraging the structured sample to spot any systemic issues (differences between strata, recurring error themes). This approach balances efficiency with insight: it **minimizes the manual review load** while maintaining statistical validity and yielding actionable information on how to improve the mapping system. By repeating this process every few months, we can confidently track the mapping performance over time and promptly detect any new problem patterns, all with a manageable sample size and rigorous methodology.

## References

* Wikipedia: *Sample size determination* – formula for sample size with margin of error
* Investopedia: *Stratified Random Sampling* – benefits of stratification for precision and representation
* Scribbr: *Sampling Methods* – definition of stratified sampling and need for subgroup representation
* DataHeroes ML Blog: *Error Analysis* – importance of analyzing misclassified samples to find error patterns


## FAQ

1. Nature of the Mappings

Q: What type of mappings are you evaluating? (e.g., terminology mappings between medical vocabularies, entity resolution, language translations, data schema mappings)
A: Mapping between medical terms extracted from healthcare notes and medical records to numerical representation using codified systems like LOINC etc. 

Q: What constitutes a single "term" and what is it being mapped to?
A: Its usually sequaence of few words representing specific medical term. E.g. for lab results the term could be 'blood sugar', or 'glucose'. For conditions domain it could be 'cancer' or 'diabetes II'.

2. Evaluation Metrics & Error Types

Q: How do you currently measure mapping accuracy? (binary correct/incorrect, or more nuanced scoring?)
A: We label each model prediction with one of the values:
    - Acceptable Specific (AS) - when model prediction is highly accurate
    - Acceptable Wide (AW) - when model prediction is acceptable, but not the best
    - Rejected (R) - when model's prediction is not acceptable

  Based on these labels we compute evaluation metrics:

    - Accuracy (Clinical  Accetability): Accuracy = (AS+AW)/(AS+AW+R)

    - Specificness (Clinical Specificness): Specificness = AS/(AS+AW)

    - Coverage: metric is used to estimate how much of the entire corpus our model could map with high confidence. Coverage = "Number of high confidence predictions" /  "Total number of terms"

Q: What types of errors are most critical to detect?
A: Inaccurate mappings

Q: Are certain types of mappings more prone to errors than others?
A: Unknown

3. Current Process & Constraints

Q: How many records can your experts realistically annotate per day/week?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: Do you have a target sample size in mind based on your resources?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: What confidence level and margin of error would be acceptable for your accuracy estimates?
A: 95% confidence level and +/-2%error  

4. Dataset Characteristics

Q: Is the error rate expected to be uniform across the dataset, or are there subgroups with potentially different error rates?
A: Unknown. We start with hypothesis that errors are distributed uniformly, but we should provide technique to sample around specific terms groups if needed. 

Q: Do you have any prior knowledge about the expected overall accuracy rate?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: Are there metadata or features that could help stratify the population?
A: Yes, assume there are some metadata fields, such as data source, timestamps and few others. 

5. Statistical Requirements

Q: Do you need to detect rare but critical errors (requiring larger samples)?
A: No

Q: Is the goal to get a point estimate of accuracy, or also to compare accuracy across different subgroups?
A: Point estimate

Q: Do you need ongoing monitoring or is this a one-time evaluation?
A: Ongoing monitoring. We will be doing this every 3-4 months.
