---
title: "v3 Designing a Sampling Strategy for Mapping Accuracy Evaluation"
date: 2025-07-10T09:00:00Z
description: ""
author: "Vladimir Kroz & gpt 4.5 in research mode"
draft: true
tags: ["AI", "ML validation", "ML Quality", "Medical", "Data"]
---


# Sampling Strategies for Efficient Mapping Accuracy Evaluation

## Problem statement

Evaluation of mapping quality requires an 'evaluation dataset', in which mappings are examined and annotated by human experts. Our challenge is that annotating data for evaluation dataset takes weeks of manual labor. The goal of this study is to determine sampling methods that produce evaluation datasets with minimal amount of records while maintaining statistical significance.
The primary goal of the sampling is twofold: (1) to estimate the overall mapping accuracy of the terms, and (2) to identify any systemic issues or patterns of errors.
Dataset Scale: Depending on domain, it ranges from 10 to 200 million terms.


## Objectives and Challenges

In evaluating mapping quality for millions of medical terms, our goals are twofold: **(1)** to estimate the overall mapping accuracy (clinical acceptability) with high statistical confidence, and **(2)** to identify any systemic error patterns. We seek to achieve these goals with the *minimal number of expert-annotated records* while maintaining statistical significance (95% confidence level, ±2% margin of error). This translates to needing a **precise estimate** of the proportion of terms correctly mapped (Acceptable Specific + Acceptable Wide) out of the total. At 95% confidence and ±2% error, a simple random sample of roughly *2,400* records is typically required for large populations. However, with smart sampling design, we may reduce this number without sacrificing precision. We also must ensure the sample is diverse enough to reveal error patterns across different term types, data sources, and model confidence levels.

**Key challenges include:**

* **Enormous Population:** The term corpus ranges from 10–200 million terms, making exhaustive labeling infeasible. We need a sampling method that yields reliable accuracy estimates from only a few thousand annotated terms (or fewer).
* **Accuracy Metric Precision:** We must attain a tight confidence interval (±2%) around the true accuracy (acceptability rate). This demands an adequate sample size and careful sampling design to control variance.
* **Error Pattern Detection:** Errors might not be uniformly distributed. Certain subgroups of terms (e.g. lab results vs. conditions, terms with low model confidence, specific data sources or time periods) could have higher error rates. The sampling strategy should ensure these sub-populations are represented, so that systemic issues can be detected.
* **Ongoing Monitoring:** The evaluation will be repeated every 3–4 months. Consistency in methodology is needed to compare accuracy over time, and the sampling must be efficient for repeated use.

## Evaluation Metrics and Statistical Significance

We categorize each model mapping as: *Acceptable Specific (AS)*, *Acceptable Wide (AW)*, or *Rejected (R)*. From these, we derive:

* **Clinical Acceptability (Accuracy):** $\text{Accuracy} = \frac{AS + AW}{AS + AW + R}$. This is the primary metric (proportion of mappings that are acceptable).
* **Clinical Specificness:** $\text{Specificness} = \frac{AS}{AS + AW}$. Measures how often an acceptable mapping is the *most specific* one.
* **Coverage:** Fraction of all terms the model maps with high confidence. (For example, if the model only produces a prediction when confident, coverage = #predicted terms / total terms.)

Our main statistical target is the overall Accuracy. We desire a 95% confidence interval for Accuracy that is ±2% in width. For an unbiased estimator (like sample proportion), the required sample size $n$ can be approximated by the standard formula for margin of error:

$n \approx \frac{Z^2 \, p(1-p)}{E^2},$

with $Z=1.96$ for 95% confidence, margin $E=0.02$, and $p$ the true proportion. In worst-case (maximum variance at $p=0.5$), this gives about **n ≈ 2,400** samples. If we expect higher accuracy (say 90%), the variance $p(1-p)$ is smaller, and the needed n might be closer to \~1,500–2,000; but without a precise prior, planning for \~2,400 ensures ±2% precision. Notably, for very large populations (millions of terms), the population size has negligible effect on required n (the finite population correction is minimal when n is a tiny fraction of N).

In addition to overall accuracy, we want the sample to enable **subgroup analysis** (e.g. by term type or confidence level) to spot patterns. However, measuring each subgroup’s accuracy with ±2% would require much larger total samples. Instead, we aim for a **representative sample** that can flag major deviations in subgroups. Statistical significance for subgroup differences can be assessed if sample allocation is done thoughtfully (e.g. stratified sampling). If an error pattern is strong (e.g. one category’s accuracy is 10% lower than others), the sample should be large enough to likely catch that difference outside the margin of error.

## Simple Random Sampling (Baseline)

**Simple Random Sampling (SRS)** selects terms uniformly at random from the entire population. This is the simplest design and provides an unbiased estimate of overall accuracy. Using SRS with \~2,400 terms would yield the desired ±2% confidence interval for accuracy. The advantages of SRS are simplicity and ease of implementation: every term has equal chance, and standard formulas apply for the estimate’s variance.

However, there are drawbacks to using SRS alone in our context:

* **Potentially Larger Sample than Necessary:** SRS does not leverage any prior information. If error rates vary across subsets (which is likely in a heterogeneous medical dataset), SRS might sample many easy cases while only a few of the harder cases, leading to sub-optimal use of annotation effort. We might end up labeling more terms than needed to achieve the precision target in the worst case.
* **Limited Error Pattern Visibility:** Because SRS allocates samples purely proportional to the population, **rare subgroups** may be under-represented. For example, if 5% of terms come from a specific source or category, an SRS of 2,400 yields only \~120 examples from that group on average. This might be marginal for identifying patterns in that subgroup (especially if only a handful turn out erroneous). If some important term category is only 1% of the corpus, SRS would sample \~24 of them – possibly too few to confidently assess that category’s accuracy.
* **No Focus on Low-Confidence Predictions:** The model likely assigns confidence scores (High/Medium/Low) to its mappings. Typically, *low-confidence predictions* have higher error rates. SRS will include relatively few low-confidence cases if those are a minority of the data, meaning we might miss many errors that concentrate in the low-confidence tail. Conversely, it will include many high-confidence cases (which might be almost always correct), yielding diminishing returns for finding errors or improving precision.

In summary, SRS is a good starting point for an unbiased overall estimate. With \~2.4k random samples, we can measure overall acceptability to ±2% error. We can also compute Specificness and other metrics on that sample. *If* errors were truly uniformly distributed, SRS would also naturally reveal error patterns (since any subgroup’s error rate would be similar to overall). But if there are hidden pockets of higher errors, SRS might not give enough samples in those pockets to notice. Thus, while we will use SRS formulas for baseline calculations, we should consider more efficient designs to meet our goals with fewer samples or more insight.

## Stratified Sampling for Greater Efficiency and Insights

**Stratified Random Sampling** involves dividing the population into distinct *strata* (subgroups) and sampling within each stratum. This approach can greatly improve the **precision** of our accuracy estimate and ensure coverage of key subgroups. The basic idea is to partition terms by factors that might influence mapping accuracy, then sample from each partition. Each stratum’s results are later combined (weighted by their population proportion) to produce overall metrics.

**Benefits of Stratification:** By guaranteeing representation of each subgroup, stratified sampling yields a more balanced and often *more precise* estimate than SRS, especially if error rates differ by stratum. It “highlights differences among groups” rather than averaging everything together. If strata are chosen such that terms within a stratum are relatively homogeneous in their mapping difficulty, and there are differences between strata, this reduces variance in the estimator. In other words, **the more mapping accuracy varies between strata (and is consistent within a stratum), the more stratification helps**.

In our context, natural stratification choices include:

* **By Model Confidence:** Partition terms into High, Medium, and Low confidence groups (based on the model’s output score or threshold). The hypothesis is that **Low-confidence predictions will have a lower accuracy** (more errors), whereas High-confidence ones will be mostly correct. This is a powerful stratification because it directly uses the model’s own signals about uncertainty. Research shows that leveraging model-predicted performance in stratification can *sharply increase precision and reduce sample size* needed. In fact, one study found that clustering data by predicted error rate and stratifying on those clusters led to up to **10× efficiency gains** in accuracy estimation compared to SRS. The intuition: rather than waste many samples on cases the model is almost certainly getting right, we allocate more samples to the “risky” cases (medium/low confidence) to pin down their higher error rates. We would then weight by the proportion of each confidence category to get overall accuracy.

  * *Sample allocation:* One can use **proportional stratification** (sample each confidence stratum in proportion to its frequency in the population) or **optimal allocation** (Neyman allocation) which gives each stratum a sample size ∝ $N_h \sqrt{Var_h}$ (i.e. proportional to population size times the standard deviation of outcome in that stratum). Neyman allocation effectively assigns more samples to strata with higher variability in outcomes. In our case, the Low-confidence stratum likely has $p(1-p)$ higher (around p ≈ 0.5 perhaps) compared to High-confidence stratum (p ≈ 0.99, very low variance), so optimal allocation would heavily weight the low-confidence group. This yields the most precise overall estimate for a given total n. Even proportional stratification often improves precision because the high-confidence majority is so homogeneous (almost all correct) that sampling it beyond a modest number is redundant information.
  * *Efficiency gains:* With stratification by confidence, we can expect to **cut down the required sample size significantly**. For example, if the model’s high-confidence predictions are 98% accurate and make up 80% of terms, and low-confidence are 80% accurate in 20% of terms, an SRS of 2,400 wastes many samples verifying what we already suspect (that most of the 80% high-conf terms are correct). A stratified design might allocate, say, 1,200 samples to the 80% high-conf stratum and 1,200 to the 20% low-conf stratum (disproportionately oversampling low-conf). The combined weighted estimator would still yield overall accuracy, but the precision would be much better because we thoroughly measured the problematic stratum. Studies confirm that stratified sampling can **“dramatically reduce the number of annotations needed”** for the same precision vs. SRS – in one experiment, nearly *5× fewer* samples were needed on one dataset, and *10× fewer* on another, by using stratification with optimal allocation. This means we might achieve ±2% accuracy margin with perhaps on the order of only a few hundred annotated terms if our stratification is very effective (though to be conservative, plan for maybe \~50% or more reduction rather than an order of magnitude in our first iteration).
  * *Pattern detection:* Importantly, this approach naturally surfaces error patterns – e.g. if low-confidence terms are often certain types of medical concepts, we will observe those in the sample and can analyze why the model struggles. We will also be able to report accuracy *within each confidence tier*, validating whether the model’s confidence calibration is sound (e.g. "high-conf predictions were 99% acceptable, low-conf were only 80% acceptable" – a useful diagnostic itself).

* **By Term Domain or Category:** We can stratify by known term categories, such as **labs vs. conditions vs. medications**, etc., or any other domain classification of the terms. If our corpus covers multiple semantic types of terms, the mapping performance might vary accordingly (for example, perhaps lab test names map easily to LOINC codes, whereas condition terms to ICD codes might be harder, etc.). By sampling each category, we ensure each domain’s accuracy can be estimated. This helps identify systemic issues (e.g., “Lab results terms have 98% acceptability, but medication terms only 90% – indicating a mapping knowledge gap in medications”). We would take a random sample from each domain stratum. For overall accuracy, we weight by each domain’s proportion in the population so the estimate isn’t biased by oversampling any domain.

  * If domain proportions are very skewed (say 90% of terms are labs, 10% conditions), a **proportional stratification** will naturally allocate most samples to labs. That yields the most precise overall accuracy (since labs dominate overall). But if we specifically want to guarantee enough samples for the smaller domain (conditions) to assess its performance, we might choose **disproportionate stratification** – i.e. deliberately oversample the smaller strata beyond their population share. For instance, instead of 90/10 split in the sample, we might do 70/30, to get more condition terms reviewed. This will slightly reduce overall estimator precision (because we oversampled a group that contributes less weight), but it improves the statistical power to identify an accuracy issue in that minority group. We can correct for the oversampling via weights in the overall accuracy calculation.
  * With domain stratification, the total sample needed for ±2% overall might or might not be lower than SRS – it depends on how different the accuracies are. If all domains have similar accuracy, stratification doesn’t reduce variance much (though it still guarantees representation). If they differ, stratification prevents one domain’s high variance from inflating overall variance. It also helps if one domain is small but has high error – SRS might completely miss that; stratification will catch it.
  * We can also stratify by **data source** (e.g. hospital A, hospital B, etc., if applicable) or **time period** (e.g. terms from 2020 vs 2023 notes) if we suspect those might influence mapping (perhaps newer data or certain institutions have different terminology). This again ensures if one source has an issue, it will be observed.

* **By Term Frequency or Novelty:** Another possible stratification is by how common a term is or by whether the model has seen similar terms in training. For example, terms could be stratified into: very frequent terms vs. infrequent/rare terms. The model might do well on frequent, familiar terms but poorly on rare ones (or vice versa if the model overfits common ones and misses edge cases). If we have metadata on term frequency or confidence that correlates, we could incorporate it. However, this might overlap largely with the confidence stratification (the model’s confidence likely is lower on unusual terms). Still, ensuring some representation of rare terms in the sample may help catch niche errors.

**Implementing Stratified Sampling:** To proceed, we would:

1. **Define Strata:** Based on the above factors, decide stratification variables. A practical approach is a *multi-factor stratification*: for instance, we might stratify by confidence level first, and within each confidence stratum ensure representation of key domains. In practice, one could create a combined stratification key (e.g. 3 confidence levels × 2 major domain categories = 6 strata) and sample from each. If that gets too granular (many strata with few samples), a simpler scheme might be better. Given the importance of leveraging model confidence, one recommendation is to stratify primarily on confidence category, and secondarily ensure domain coverage by maybe doing proportional allocation within each confidence stratum for each domain.
2. **Allocate Sample per Stratum:** Decide on proportional vs. optimized allocation. For a first evaluation, proportional allocation to population share is straightforward and preserves an unbiased overall estimator. For example, if 70% of terms are high-confidence, 20% medium, 10% low, then out of a total sample of \~2,400, we’d take \~1,680 high-conf, 480 medium, 240 low (this is proportional). If using Neyman optimal allocation, we’d need prior estimates of error variance in each stratum (perhaps from a pilot or from model's self-estimates). If we suspect, say, low-conf has p \~0.7 (variance .21) and high-conf p \~0.98 (variance .02), the Neyman formula would skew even more samples to low and medium. We might then sample somewhat disproportionately more low/medium than their share. A conservative compromise is to at least **set a minimum sample size for smaller strata** to get reliable insight (e.g. ensure at least 200 low-confidence samples even if they form only 5% of data, which would be disproportionate since 5% of 2400 is just 120; this ensures enough errors in that stratum to analyze). As long as we weight by true stratum proportions in final analysis, the overall accuracy estimate remains unbiased.
3. **Conduct Sampling and Annotation:** Within each stratum, we randomly select the required number of terms for expert annotation. Since terms are being mapped likely without additional context (though possibly context could be provided), this random selection within homogeneous groups avoids any selection bias.
4. **Estimation:** Compute the acceptability rate for each stratum from the annotations, then combine: overall accuracy = ∑(weight\_h \* accuracy\_h) where weight\_h = fraction of population in stratum h. Compute the confidence interval using stratified sampling formulas (which account for within-stratum variance and the sample sizes in each). Typically, stratified estimation yields a smaller standard error than an equivalently sized SRS if stratification was effective. We will have not only the overall metrics but also metrics per stratum (e.g. accuracy among low-conf = X%, among high-conf = Y%). These can directly indicate error patterns (e.g. “low confidence terms had a 20% rejection rate vs only 2% for high confidence terms” or "mapping accuracy in Lab terms is 99% vs 90% in Condition terms").

**Expected Sample Size and Precision:** By using stratification, especially on model confidence, we anticipate needing significantly fewer than 2,400 total samples to achieve the same ±2% precision. Recent research demonstrated cases with **5× to 10× fewer samples** for the same precision by clever stratification and estimation. For instance, if SRS of 2,400 yields a 2% margin, a stratified design might achieve that margin with perhaps on the order of \~800–1200 samples (this will depend on how skewed and predictive the chosen strata are). Even if we are conservative and still sample \~2,000, the stratified estimator’s margin of error will be tighter than 2% – giving us more confidence. We should highlight that while stratification can **reduce statistical uncertainty** (i.e. narrower confidence interval for a given n), the **actual number of annotation tasks** might also be informed by practical considerations (like minimum needed per group to analyze errors qualitatively).

Given our dual goal, it might be wise to allocate a bit more sample than the bare minimum for the interval, to have material for error analysis. For example, maybe we target \~2,000 stratified samples which should comfortably give <2% margin, and yields a reasonable count of errors to inspect (if accuracy \~95%, 2,000 samples might find \~100 errors total, spread across strata to examine for patterns). If accuracy is higher, we might increase n or oversample lower-accuracy strata to capture enough errors to analyze root causes.

## Sampling for Systematic Error Detection

Beyond estimating the overall accuracy, we need to **identify systemic error patterns**. Stratified sampling as described already aids this by ensuring different groups are represented. In addition, we can employ targeted sampling techniques specifically to surface errors (though these must be balanced with maintaining unbiased estimates):

* **Disproportionate (Focused) Sampling:** As mentioned, deliberately oversampling certain subgroups of interest can help find errors in those subgroups. For example, if “rare terms” or certain specialty terminology are suspected trouble spots, we can sample a higher fraction of those than their occurrence rate. This is a form of *judgmental* tilt but done within a probabilistic framework (we know the fraction oversampled, so we can adjust the weights for accuracy calculation). This approach does not minimize variance of the overall estimate, but it maximizes the information about that subgroup’s issues. For instance, if only 0.5% of terms come from pediatrics, an SRS of 2,000 would yield about 10 pediatric terms – too low to say anything. We could instead sample, say, 50 pediatric terms. While 50 is still a tiny fraction of 2,000, it’s a 5× oversample of that subgroup. We then weight them appropriately (each pediatric term in sample represents 0.5%/2.5% = 0.2 of a population percentage, for instance) when computing overall accuracy. This way, overall accuracy remains unbiased (with a slight variance penalty for unequal weights), but we now have a large enough subsample to calculate pediatric term accuracy with a reasonable margin and see if it’s lower.
* **Recursive or Two-Phase Sampling:** Another strategy is to do an initial broad sample, identify patterns, and then perform a second-phase sample focusing on any suspect areas. Statistically, this can be viewed as a form of two-phase sampling or adaptive sampling. For example, an initial sample of 1,000 might give a rough overall accuracy and highlight that (say) oncology-related terms seem to have many errors. Then one could sample an additional 200 oncology terms to investigate further. The second sample could primarily serve qualitative error analysis, while the first sample handles the overall metrics. If needed, both can be combined (with appropriate weighting) to refine the quantitative estimate for that subgroup or overall. Since the question emphasizes *statistical methods* and not active learning, this adaptive approach would still follow predefined rules (like “if any stratum’s error rate estimate is below X, bolster that stratum with more samples”).
* **Monitoring Over Time:** Because this evaluation will recur periodically (every 3–4 months), we can also leverage past results to refine sampling. For example, if previous rounds consistently showed one category with lower accuracy, we might stratify on that category in future rounds to track it more closely. Additionally, for ongoing monitoring, maintaining some *consistent stratification scheme* each time is useful so that results are comparable. We may also use a **cumulative approach**: over time, build a knowledge base of error patterns so that sampling can increasingly focus on new or unresolved areas (while still keeping enough baseline random sampling to measure overall performance changes).

In summary, to detect error patterns, **stratification by relevant metadata and model outputs** is key. It ensures we get a spread of examples across potential problem dimensions. We will analyze the annotated sample by these dimensions (confidence level, term type, source, etc.) to see if any group has a significantly higher rejection rate. If yes, that signals a systemic issue in the mapping logic for that group. The statistical significance of such differences can be tested (e.g. a chi-square or z-test for two proportions comparing strata), though with our sample sizes, even moderate differences will likely be detectable. If an issue is found, we might adjust future samples to focus more on that area (to verify improvement after fixes, etc.).

## Cluster Sampling Considerations

Another sampling approach to consider is **Cluster Sampling**, where instead of sampling individual terms independently, we sample clusters of terms (and then include all terms within selected clusters for evaluation). Clusters could be natural groupings like *all terms from a given document or patient*, or *all terms from a certain hospital*, etc. Cluster sampling can be logistically efficient: for example, if terms require context from the source document for accurate human annotation, pulling an entire document and having the expert review all terms in it might save time compared to jumping randomly between documents for each term. It also can reduce the overhead if data is stored or accessed cluster-wise.

However, **cluster sampling has a major statistical drawback**: it tends to reduce the precision of estimates unless a large number of clusters are sampled. Terms in the same cluster are often more homogeneous (correlated) in their mapping outcomes. For instance, if one clinical note uses very idiosyncratic language, many terms in that note might all be hard (or all easy) to map. By taking them together, you get less independent information than if you had sampled the same number of terms from entirely different notes. In sampling terms from clusters, the *effective sample size* is lower than the raw number of terms because of intra-cluster correlation. In general, **clustering increases the variance (design effect) of estimates** – *“clustering typically increases the design effect (and decreases the effective sample size)”*. The more homogeneous the clusters, and the larger each cluster, the more the variance inflation. In practical terms, **cluster sampling requires a larger total sample to achieve the same ±2% margin** compared to SRS or stratified sampling. For example, if terms are clustered by document and within each document the mapping accuracy is either almost entirely good or bad, then to average that out, you need many documents. A rule of thumb uses the **intraclass correlation (ICC)**: if ICC is \$\rho\$ and cluster size is m, the variance is inflated by ≈ \$1 + (m-1)\rho\$. Even a modest ICC of 0.1 with cluster size 10 yields a design effect of 1.9 (\~90% more samples needed). Thus, cluster sampling is *less statistically efficient* – it **"needs larger sample sizes to ensure accuracy"** to compensate for cluster homogeneity.

Given our priority is minimal sample size, cluster sampling is **not ideal** unless necessary for practical reasons. If expert annotation *requires* context that makes clustering advantageous (e.g. an expert can annotate 10 terms from one record in the time it takes to do 5 terms from separate records), then cluster sampling might reduce cost/time even though it increases variance. In such a case, we would treat each cluster (e.g. each document) as a sampling unit and sample \~n clusters, then evaluate all terms in them. We would then use cluster-adjusted statistical analysis to estimate accuracy. The sample size (in terms of terms) would need to be higher. For instance, to get the equivalent of \~2,400 independent terms, we might need to sample say 300 documents with on average 10 terms each, if the design effect is around 1.25–1.5. We would compute the confidence interval accounting for cluster variance (using, e.g., the Rao-Scott adjusted methods or treating clusters as primary sampling units). If possible, we should keep clusters small (e.g. sample many small notes rather than a few large ones) to minimize intra-cluster correlation effects. Also, stratification can be combined with clustering (stratify by type of document, then cluster-sample within each, etc.), but this complicates matters further. Unless the annotation workflow dictates clustering, we likely prefer *simple or stratified random sampling of individual terms*.

To summarize cluster vs others:

* **Simple Random Sample:** Statistically efficient (each term independent), \~2,400 needed for 95%±2%. But may not cover all subgroups well.
* **Stratified Sample:** Statistically **most efficient** when strata are well-chosen; can reach same precision with fewer terms (potentially 50% or even 10% of SRS sample in best cases). Ensures representation of subgroups and yields insight into error patterns by group. Our recommendation is to use stratification heavily (by confidence and possibly by domain/source).
* **Cluster Sample:** *Least* statistically efficient (higher uncertainty per sample) – requires larger samples for same precision. Use only if needed for convenience/cost reasons (e.g. grouping by record to speed up annotation), and even then, prefer many clusters with few terms each. Stratification can and should still be applied at the cluster level if using this (e.g. ensure you pick clusters from different sources or confidence categories proportionally).

## Recommended Sampling Strategy

Considering all factors, **our primary recommendation is a stratified random sampling approach**, leveraging the model’s confidence scores and relevant metadata, to construct an evaluation dataset that is both small and information-rich. Below is a concrete plan:

* **1. Stratify by Model Confidence (High/Medium/Low):** This is expected to yield the biggest efficiency gain. It ensures the sample includes the challenging cases in adequate numbers. We will use proportional or slightly disproportionate allocation favoring lower confidence strata to maximize precision. For instance, if the model labels 60% of terms as high-confidence, 30% medium, 10% low, we might allocate roughly 50% of samples to high, 30% to medium, 20% to low – oversampling the rare low-confidence cases because they likely contribute outsized error. This stratification directly addresses the *coverage* metric as well: we will be able to validate that high-confidence predictions are indeed >X% accurate (and thus truly “high quality coverage” of the corpus).
* **2. Stratify (or post-stratify) by Term Category/Source:** We will ensure representation of major term categories (labs, conditions, etc.) or data sources. This could be done via a second-level stratification or by ensuring the sample is drawn in a way that covers these groups. One pragmatic way: within each confidence level stratum, perform a proportional allocation across domain types. For example, within the low-confidence group, make sure we sample low-conf lab terms and low-conf condition terms proportionally. If some categories are very small (and risk not appearing in the sample), we will explicitly sample a minimum from them (even if it’s an oversample) to check their performance.
* **3. Sample Size:** Start with a **target of \~2,000 total terms** to annotate, allocated across strata as per above. This number is slightly lower than the SRS 2,400, banking on stratification efficiency. Based on literature and assumptions, 2,000 well-chosen stratified samples should achieve the ±2% precision (likely even ±1.5% or better). If early analysis of variance suggests we could reduce further, we might attempt \~1,500 in subsequent rounds, but initially 2k provides a safety margin to also allow comfortable subgroup analysis. These 2,000 could be, for example: 1000 high-conf, 600 medium, 400 low-conf (oversampling low); within each of those, if we have domain sub-strata, ensure e.g. at least 100 condition terms, 100 lab terms, etc., even if it means slightly shifting the distribution.
* **4. Analysis:** Compute overall accuracy and specificness with stratified weighting. The 95% CI will be derived from stratified variance formulas. We expect a tight CI given this design (e.g. if actual accuracy \~95%, the 95% CI might be around 93%–97% or narrower). Additionally, compute accuracy within each confidence level, and each domain. We will flag any significant drop in a subgroup (for instance, if medium-confidence condition terms have only 80% acceptability whereas others are 95+%, that’s a red flag). Because each subgroup will have a decent sample size in this design, we can be confident in observed differences – they won’t be mere noise.
* **5. Systemic Error Investigation:** Use the stratified sample’s annotations to perform error analysis. Identify common themes among the *Rejected (R)* cases: are they coming disproportionately from certain term types? For example, maybe many rejections are for highly polysemous terms (e.g. "cancer" without context mapped incorrectly), or for numeric ranges, etc. The sample is designed to uncover such patterns by ensuring those terms were in scope. If needed, follow-up sampling can be done for any newly discovered problem area (though likely the next cycle of evaluation will check if fixes improved those).
* **6. Repeat and Refine:** For subsequent evaluations (in 3–4 months), reuse the stratified approach so results are comparable. We might adjust strata boundaries or allocation based on previous findings (for instance, if we found the model is nearly perfect on high-conf terms, we might even reduce sampling there further next time in favor of more low-conf sampling to monitor improvements). Over time, this approach will continue to give a solid point estimate of accuracy and highlight trends (e.g. overall accuracy rising or falling, specificness improving, etc.), all with minimal labeling effort.

In conclusion, **statistical stratified sampling** is the optimal solution for our needs. It provides the best of both worlds: **minimal sample size for a given confidence level** by reducing variance (selecting the most informative instances), and **comprehensive coverage of potential error sources** by structured representation. Simple random sampling, while unbiased, would likely require more samples (\~2.4k) and risk missing niche issues. Cluster sampling, while sometimes logistically convenient, would require an even larger sample to reach ±2% precision and thus is not recommended unless necessary. By leveraging known information (model confidence and metadata) in a statistically principled way, we ensure our evaluation dataset is *small but mighty* – yielding an accurate overall accuracy measurement at 95% confidence, and shining light on any systematic mapping errors, all with the least manual review effort.

## Comparative Summary

To clearly compare the methods in terms of sample size and power, consider:

* **Simple Random Sampling:** Requires \~2,400 annotations for 95% CI ±2%. No grouping information used, so precision is baseline. Subgroup analysis possible but smaller groups have high uncertainty.
* **Stratified Sampling (proposed):** Likely achieves the same precision with significantly fewer annotations (potentially on the order of 1,500 or even fewer) by exploiting variance differences. For example, simulations in literature show **5×–10× fewer samples** needed in best cases – in our case even a conservative 20–30% reduction is valuable. Also provides higher **power** to detect differences between subgroups because those groups are deliberately sampled (e.g. we can detect a 5% accuracy difference between two term types with decent confidence if each has, say, 300+ samples in the stratified design, whereas SRS might have only 50 from a small type, too few to be sure). Stratification thus increases the *statistical power to identify patterns* without increasing total sample, which directly addresses goal (2).
* **Cluster Sampling:** Typically requires a larger sample to compensate for intra-cluster correlations. For instance, if clustering by document, one might need 3,000–5,000 terms (depending on cluster size and correlation) to achieve the same ±2% confidence as 2,400 SRS. It has *lower power per annotated term* because many terms may share the same context (hence similar outcomes). It is best used only if it substantially saves annotation effort in practice. If used, one might do something like sample \~300 documents and annotate all terms within, then use a clustered estimator for accuracy. But again, this is statistically less efficient and thus not our primary recommendation.

All things considered, **using stratified random sampling with a total of roughly 2,000 annotated terms** (adjusted as we learn more) is our recommended path. This approach is firmly grounded in statistical theory and recent research demonstrating improved efficiency in model evaluation. It will allow us to confidently estimate the overall mapping accuracy at the 95%±2% level, and concurrently surface any systematic errors, thereby maximizing the value of each expert-annotated term in the evaluation dataset.

**Sources:**

* Fogliato et al., *"A Framework for Efficient Model Evaluation through Stratification, Sampling, and Estimation"* – demonstrates that stratified sampling (especially using model predictions) can drastically improve precision, with up to 10× fewer annotations needed for the same accuracy estimate precision.
* Investopedia (Stratified Sampling) – emphasizes that stratification yields more precise estimates by better representing subgroups in the sample.
* Stats texts on design effect – note that stratification generally reduces variance while clustering increases it, meaning stratified designs are more statistically power-efficient, whereas cluster designs need larger samples.
* SurveyPractice article – confirms the sample size (\~2,400) for 95% CI ±2% in a proportion estimate, which we used as a baseline for planning the evaluation dataset size.



## FAQ

1. Nature of the Mappings

Q: What type of mappings are you evaluating? (e.g., terminology mappings between medical vocabularies, entity resolution, language translations, data schema mappings)
A: Mapping between medical terms extracted from healthcare notes and medical records to numerical representation using codified systems like LOINC etc. 

Q: What constitutes a single "term" and what is it being mapped to?
A: Its usually sequaence of few words representing specific medical term. E.g. for lab results the term could be 'blood sugar', or 'glucose'. For conditions domain it could be 'cancer' or 'diabetes II'.

2. Evaluation Metrics & Error Types

Q: How do you currently measure mapping accuracy? (binary correct/incorrect, or more nuanced scoring?)
A: We label each model prediction with one of the values:
    - Acceptable Specific (AS) - when model prediction is highly accurate
    - Acceptable Wide (AW) - when model prediction is acceptable, but not the best
    - Rejected (R) - when model's prediction is not acceptable

  Based on these labels we compute evaluation metrics:

    - Accuracy (Clinical  Accetability): Accuracy = (AS+AW)/(AS+AW+R)

    - Specificness (Clinical Specificness): Specificness = AS/(AS+AW)

    - Coverage: metric is used to estimate how much of the entire corpus our model could map with high confidence. Coverage = "Number of high confidence predictions" /  "Total number of terms"

Q: What types of errors are most critical to detect?
A: Inaccurate mappings

Q: Are certain types of mappings more prone to errors than others?
A: Unknown

3. Current Process & Constraints

Q: How many records can your experts realistically annotate per day/week?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: Do you have a target sample size in mind based on your resources?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: What confidence level and margin of error would be acceptable for your accuracy estimates?
A: 95% confidence level and +/-2%error  

4. Dataset Characteristics

Q: Is the error rate expected to be uniform across the dataset, or are there subgroups with potentially different error rates?
A: Unknown. We start with hypothesis that errors are distributed uniformly, but we should provide technique to sample around specific terms groups if needed. 

Q: Do you have any prior knowledge about the expected overall accuracy rate?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: Are there metadata or features that could help stratify the population?
A: Yes, assume there are some metadata fields, such as data source, timestamps and few others. 

5. Statistical Requirements

Q: Do you need to detect rare but critical errors (requiring larger samples)?
A: No

Q: Is the goal to get a point estimate of accuracy, or also to compare accuracy across different subgroups?
A: Point estimate

Q: Do you need ongoing monitoring or is this a one-time evaluation?
A: Ongoing monitoring. We will be doing this every 3-4 months.

