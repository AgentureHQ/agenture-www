---
title: "v0 Designing a Sampling Strategy for Mapping Accuracy Evaluation"
date: 2025-07-10T09:00:00Z
description: ""
author: "Vladimir Kroz & gpt 4.5 in research mode"
draft: true
tags: ["AI", "ML validation", "ML Quality", "Medical", "Data"]
---


# Designing a Sampling Strategy for Mapping Accuracy Evaluation

## Problem statement

Evaluation of mapping quality requires an 'evaluation dataset', in which mappings are examined and annotated by human experts. Our challenge is that annotating data for evaluation dataset takes weeks of manual labor. The goal of this study is to determine sampling methods that produce evaluation datasets with minimal amount of records while maintaining statistical significance.
The primary goal of the sampling is twofold: (1) to estimate the overall mapping accuracy of the terms, and (2) to identify any systemic issues or patterns of errors.
Dataset Scale: Depending on domain, it ranges from 10 to 200 million terms.

## FAQ

1. Nature of the Mappings

Q: What type of mappings are you evaluating? (e.g., terminology mappings between medical vocabularies, entity resolution, language translations, data schema mappings)
A: Mapping between medical terms extracted from healthcare notes and medical records to numerical representation using codified systems like LOINC etc. 

Q: What constitutes a single "term" and what is it being mapped to?
A: Its usually sequaence of few words representing specific medical term. E.g. for lab results the term could be 'blood sugar', or 'glucose'. For conditions domain it could be 'cancer' or 'diabetes II'.

2. Evaluation Metrics & Error Types

Q: How do you currently measure mapping accuracy? (binary correct/incorrect, or more nuanced scoring?)
A: We label each model prediction with one of the values:
    - Acceptable Specific (AS) - when model prediction is highly accurate
    - Acceptable Wide (AW) - when model prediction is acceptable, but not the best
    - Rejected (R) - when model's prediction is not acceptable

  Based on these labels we compute evaluation metrics:

    - Accuracy (Clinical  Accetability): Accuracy = (AS+AW)/(AS+AW+R)

    - Specificness (Clinical Specificness): Specificness = AS/(AS+AW)

    - Coverage: metric is used to estimate how much of the entire corpus our model could map with high confidence. Coverage = "Number of high confidence predictions" /  "Total number of terms"

Q: What types of errors are most critical to detect?
A: Inaccurate mappings

Q: Are certain types of mappings more prone to errors than others?
A: Unknown

3. Current Process & Constraints

Q: How many records can your experts realistically annotate per day/week?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: Do you have a target sample size in mind based on your resources?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: What confidence level and margin of error would be acceptable for your accuracy estimates?
A: 95% confidence level and +/-2%error  

4. Dataset Characteristics

Q: Is the error rate expected to be uniform across the dataset, or are there subgroups with potentially different error rates?
A: Unknown. We start with hypothesis that errors are distributed uniformly, but we should provide technique to sample around specific terms groups if needed. 

Q: Do you have any prior knowledge about the expected overall accuracy rate?
A: This is irrelevant for your task. You must focus on optimal sampling methods. This one and only goal of your work.

Q: Are there metadata or features that could help stratify the population?
A: Yes, assume there are some metadata fields, such as data source, timestamps and few others. 

5. Statistical Requirements

Q: Do you need to detect rare but critical errors (requiring larger samples)?
A: No

Q: Is the goal to get a point estimate of accuracy, or also to compare accuracy across different subgroups?
A: Point estimate

Q: Do you need ongoing monitoring or is this a one-time evaluation?
A: Ongoing monitoring. We will be doing this every 3-4 months.

## Objectives of the Sampling Evaluation

The primary goal of the sampling is **twofold**: (1) to estimate the overall mapping accuracy of the terms, and (2) to identify any systemic issues or patterns of errors. In practice, this means the sample needs to be representative enough to yield a reliable accuracy **percentage**, while also being structured to reveal if certain categories of data are performing worse than others. By addressing both objectives, we ensure the evaluation not only provides a single accuracy metric but also uncovers **which areas need improvement**. This dual focus will guide how we choose the sample and analyze the results. For example, the sampling plan should allow calculation of an overall accuracy rate with statistical confidence, *and* it should highlight any subsets of mappings that consistently fall below that overall accuracy. Achieving both aims in one study requires careful design of the sampling methodology from the outset.

## Confidence Level and Margin of Error Considerations

We will start with a target **95% confidence level** and a **±2% margin of error** for estimating the mapping accuracy. This means we want to be 95% sure that the true accuracy of all mappings lies within 2 percentage points of the accuracy observed in our sample. Such a high confidence and tight margin demand a relatively **large sample size**, since smaller margins of error require larger samples. For instance, a rough calculation shows that about *1,000* sampled mappings might yield a \~3% margin of error, whereas doubling the sample size to *2,000* can tighten the margin to about ±2%. We will determine the exact sample size using standard formulas for proportion confidence intervals, conservatively assuming around 50% to maximize required size (this is a worst-case assumption for variance). If we expect the system’s accuracy to be quite high (e.g. 90%+), the required sample could be a bit smaller, but it’s prudent to err on the larger side initially to ensure ±2% precision.

* **Initial Target (95% ±2%)**: This gives us a strong baseline confidence in the accuracy measurement. With 95% confidence, if our sample shows (for example) 93% of mappings are correct, we can be quite certain the true accuracy is between 91% and 95%. The ±2% margin is a stringent threshold, chosen to detect relatively small changes or differences in accuracy over time. Achieving this likely means sampling on the order of a couple thousand mapping instances from the system (assuming a large population of mappings).
* **Incremental Improvement and Raising the Bar**: We plan to **raise the statistical bar** over time once the process is stable. “Raising the bar” could mean increasing the confidence level (toward 99%) or tightening the margin of error (to ±1% or so) for future assessments. Either change will further increase the needed sample size, since, for example, moving from 95% to 99% confidence (with the same ±2% margin) would require a substantially larger sample, and likewise halving the margin of error from 2% to 1% would roughly quadruple the sample size needed. We will likely start with the manageable 95%/2% goal and, as the team and process mature, **scale up** the sampling effort to meet more rigorous statistical criteria. This phased approach ensures that initial evaluations are feasible, and subsequent ones become more demanding as we strive for near-perfect mapping accuracy.

*Why 95% confidence and ±2% margin?* – At the outset, these values provide a good balance between **practicality and rigor**. A 95% confidence level is standard in quality measurement, and a 2% margin of error is tight enough to catch meaningful deviations in accuracy without being unreasonably costly in terms of sample size. By improving the system and possibly expanding automated checks, we can later justify collecting even more samples to push for 99% confidence or finer precision. In summary, the sampling strategy initially targets 95% confidence with ±2% precision, with flexibility to become even stricter as incremental improvements in the system are achieved.

## Stratification and Subgroup Representation

To not only measure overall accuracy but also **identify systemic issues**, it is critical that the sample adequately represents all meaningful subgroups of the data. If certain categories of mappings (for example, specific types of lab tests, particular data sources, or patient demographic groups) are handled differently by the system, their error rates might differ. A **stratified sampling** approach will help ensure each relevant subgroup is included sufficiently in the evaluation. Stratified random sampling involves dividing the population of mappings into strata (subgroups) and sampling from each stratum, rather than sampling purely at random from the whole pool. This method *“ensures that each subgroup of a given population is adequately represented within the whole sample”*. The benefit is twofold: we get a more precise overall estimate by avoiding over- or under-sampling certain groups, and we can **assess accuracy within each subgroup** to spot problem areas.

* **Identifying Strata**: We will start by listing any known dimensions that might influence mapping accuracy. For example, **lab test types** (chemistry vs. hematology tests, etc.), **data source systems** (each source might format or code data differently), and **patient demographics** (if relevant to mapping, perhaps language or regional differences in test naming) are potential strata. If we know, for instance, that our system handles blood tests very well but struggles on genetic test codes, we should treat those categories separately in sampling. Similarly, if some data sources use unconventional coding, we want to ensure those sources are not lost in a purely random sample. By explicitly stratifying, we guarantee that even smaller subgroups (like rare test types) get *some* representation in the sample. This prevents the situation where a simple random sample might accidentally mostly cover easy cases and miss a troublesome category entirely.
* **Proportionate vs. Disproportionate Sampling**: In a proportional stratified sample, each subgroup is sampled roughly in proportion to its frequency in the population. For example, if 30% of all mappings come from Source A and 70% from Source B, then out of 2,000 samples we’d take about 600 from A and 1,400 from B. This maintains an unbiased overall estimate. However, if a subgroup is **very small but critical**, we might use a disproportionate approach – deliberately oversampling that subgroup to get enough data about it. We can still compute overall accuracy by weighting the results according to each subgroup’s actual proportion if needed. The key is to **ensure no important subgroup is glossed over**. Stratification will also allow calculation of separate accuracy metrics per stratum (with larger margins of error unless the subgroup sample itself is large), which can highlight systemic issues in one area even if overall accuracy looks fine.

In summary, stratifying the sample by known meaningful categories will improve the **representativeness and diagnostic power** of the evaluation. It aligns with best practices of ensuring all parts of the population are reflected, as *stratified sampling often provides more precise estimates by ensuring representation across subgroups*. We will define the strata based on domain knowledge of the mapping system. If uncertainty exists about what subgroups matter, we will take an additional step described next.

## Discovering Subgroups via Unsupervised Clustering

If we are **not sure what the meaningful strata are**, or suspect there are hidden groupings in the data affecting mapping outcomes, we can use unsupervised learning to help identify subgroups. Unsupervised clustering algorithms (like k-means or hierarchical clustering) can reveal natural clusters of mapping instances that share similar characteristics. The idea is to see if the mappings “**naturally partition into subgroups**” based on their attributes. For example, clustering might group lab tests by their naming conventions or complexity, or cluster mappings by the confidence score that the system’s algorithm assigns, etc. These clusters, once identified, can inform our sampling strata or at least ensure we include representatives from each cluster.

* **When to Use Clustering**: We will consider clustering in the exploratory phase, *before finalizing the sampling plan*. If the dataset is large and complex, running a clustering (or principal component analysis) on relevant features can highlight dominant patterns. *“If the goal is to get an overview of a dataset, to see what the strongest patterns are and whether the samples naturally partition into subgroups, an unsupervised method like clustering or PCA should be used.”*. In our context, this could show, for instance, that there are a few distinct clusters of lab tests (perhaps routine tests vs. specialized tests) or differences between data from different labs/hospitals that we weren’t aware of.
* **Using Clusters as Strata**: Suppose clustering reveals a particular group of mappings that has unique characteristics (for example, a cluster might be all tests whose names did not match standard codes closely, implying the system’s algorithm had to guess). We would then ensure that our sample includes an appropriate number of items from each such cluster. Essentially, each cluster can be treated as a stratum for sampling purposes. This approach is **data-driven stratification** – instead of predefining groups, we let the data suggest them. It is especially useful if the system is new or the data is aggregated from many sources where the error patterns are not yet known.
* **Caveats**: While clustering can uncover hidden structure, we must be careful in interpreting the results. The clusters are based on similarity in input features, not directly on error rates (since we won’t know errors until we actually check the sample). However, if the clustering is done on features that likely correlate with mapping difficulty (e.g. text similarity scores, or presence of certain keywords, or source identifiers), it can be a good proxy for identifying potential trouble spots. After sampling and reviewing errors, we might find that “Cluster X” had a significantly lower accuracy than others – thus confirming a systemic issue for that group. In future iterations, that cluster would warrant special attention and possibly its own improvement efforts.

By combining domain knowledge with unsupervised clustering insights, we can define strata that are **meaningful, comprehensive, and not overly granular**. The end result is a stratified sampling plan that covers both known categories and any data-driven clusters, positioning us to detect systemic issues if they exist. This answers the question of subgroups: *yes, there likely are meaningful subgroups (even if we have to discover them), and we will ensure they are well represented in the sample.*

## Sampling Methodology for Continuous Monitoring

The described sampling method is intended to be **repeatable for periodic assessments**, not a one-off exercise. The plan is to conduct this deep accuracy evaluation **every 3–4 months**. By doing so, we establish an ongoing monitoring process that tracks the system’s mapping accuracy over time and verifies that improvements are actually raising the accuracy (or that it remains high as data evolves). A periodic sampling approach aligns with best practices in data quality management: a data quality assessment or audit should be *“repeated periodically, to ensure that data quality standards are being met and maintained or improved.”* In other words, we will treat mapping accuracy like a quality metric that needs continuous oversight.

Key considerations for making the sampling method work for **repeated assessments**:

* **Consistency in Method**: Each cycle (quarterly evaluation) should use a consistent methodology so that results are comparable over time. This means we’ll keep the confidence level and margin of error targets the same (or only adjust them in a planned way as we raise standards), use the same stratification or clustering-informed strata, and similar sample selection procedures. Consistency ensures that if the accuracy moves from, say, 93% in one quarter to 96% in the next, we can trust that this reflects real improvement rather than a change in measurement approach.

* **Monitoring Trends and Variations**: By plotting the accuracy results from each periodic sample, we can observe trends. For example, if accuracy is steadily climbing upward as expected with fixes, that’s a good sign. If it plateaus or drops at any point, the team can investigate why. This approach is akin to using quality control charts in manufacturing or survey tracking in polling – it lets us distinguish normal fluctuations from significant changes over time. In fact, incorporating the results into a simple control chart or dashboard could help visualize whether changes fall outside the expected variability. We will look for any **special-cause variations** – e.g. a sudden dip in accuracy for one category might indicate a new systemic issue or data change that occurred.

* **Adjusting Over Time**: The sampling plan itself can be refined as we learn from each cycle. If in the first deep assessment we discover certain subgroups have much lower accuracy, we might decide to allocate relatively more samples to those areas in future rounds (to closely monitor improvement there). Also, if new lab test types or data sources come online in the system between assessments, we’ll incorporate those into the strata for the next sample. The process is thus **adaptive**. We maintain the rigorous statistical framework (confidence levels, etc.) but remain flexible in *what* we sample more of, depending on risk areas. Over time, as overall accuracy improves, we might also implement the earlier idea of raising the statistical bar – for instance, shifting to 99% confidence or aiming for a ±1.5% margin – to keep pushing for excellence. Each cycle will produce a report of findings (accuracy overall and by subgroup, plus recommendations), which feeds into system improvements and the plan for the next cycle.

* **Efficiency and Practicality**: Since this is a recurring effort, we will streamline the sampling and evaluation process as much as possible. This could involve developing tools or scripts to randomly select the required sample from each stratum, and perhaps partially automating the checking of mapping correctness (where feasible) to reduce manual labor. We might not manually review 2,000 mappings in one go without some automation or division of labor, so planning for how to execute the sampling review efficiently is important for sustainability. Doing this every 3-4 months means it should be treated as part of the regular quality workflow, possibly with dedicated time or personnel. As accuracy improves, it’s possible the sample size might need to increase (if we tighten the error margin), but hopefully the increased accuracy also means fewer issues to document, making review faster per item.

By institutionalizing this periodic sampling, we create a **feedback loop for continuous improvement**. The expectation is that each round of sampling will show incremental improvements in accuracy (as errors found in the previous round have been addressed). The statistical rigor (95% confidence, ±2% MoE initially) ensures that we can trust these improvements are real. And the stratified/clustering-informed design ensures that improvements (or regressions) in any particular subgroup are noticed, not hidden in the aggregate. In essence, the sampling method doubles as both a **measurement tool and a monitoring control** for the mapping system’s performance over time. It will help leadership see the quality trend and help the technical team pinpoint where to focus next.

## Conclusion

In summary, our sampling strategy is designed to **reliably measure mapping accuracy and uncover systemic issues simultaneously**. We will use a statistically robust sample (initially targeting 95% confidence with ±2% precision) to gauge overall accuracy, which demands on the order of a couple thousand instances in the sample for a large dataset. We’ll incorporate **stratified sampling** to ensure all key subgroups (e.g. various lab test categories, data sources, etc.) are represented – an approach that provides more precise and insightful results by not missing any corner-cases. Where the meaningful subgroups are not obvious, we’ll leverage **unsupervised clustering** to discover natural groupings in the data, allowing a data-driven way to stratify and catch hidden problem areas. The sampling methodology is built for **repeatability**: conducting the assessment every 3-4 months enables continuous monitoring and validation of improvements, aligning with data quality best practices of periodic audits. Each iteration will use a consistent approach so that we can track trends reliably, while also being adjusted as needed to focus on newly identified risk areas or to tighten statistical confidence as our capability grows.

By following this approach, we will not only know the system’s mapping accuracy at a high confidence level, but also gain actionable insights into **which strata or clusters of data need attention**. Over time, this will drive targeted improvements to the mapping system. The iterative nature (quarterly checks) means the bar can be raised continuously – for instance, moving toward higher confidence levels or smaller margins of error – ensuring that quality improvement is an ongoing journey. Ultimately, this rigorous sampling strategy provides a foundation for **evidence-based enhancements** to the system, giving stakeholders confidence in the reported accuracy and a clear direction for corrective measures where needed.

**Sources:** The plan and its statistical underpinnings are informed by standard survey sampling theory (for confidence levels and margins of error), best practices in stratified sampling to ensure subgroup representation, and data quality management principles emphasizing regular audits for continuous improvement. Unsupervised clustering is highlighted as a useful exploratory tool to identify data-driven subgroups when domain knowledge is limited. These approaches together form a comprehensive strategy for deep assessment and ongoing monitoring of mapping accuracy.

===========================================


# Clarification #1: 

Here's a concise **crash course** on the concepts behind the variables **Z** and **p**, the reasoning behind them, and how they connect to sampling.

## Understanding Z, p, and Sample Size

When you have a large dataset (like your healthcare data with \~200 million records), checking **every single record** is impossible. Instead, you select a smaller set (**sample**) to estimate the accuracy or quality of the entire dataset.

When you take a sample, there are two key questions:

* **How confident can we be** in the accuracy we measure from this sample?
* **How precisely** does this sample accuracy represent the real accuracy in the entire dataset?

The variables **Z** and **p** help answer these questions.

## What does Z mean? (Confidence Level)

**Z** (the "Z-value") comes from a statistical method called a **confidence interval**, which tells us how confident we can be in our estimation.

* Imagine your sample shows an accuracy of **90%**. That’s your **estimate**.
* But because you haven't measured **every record**, there's always some uncertainty. Your true accuracy could be a bit higher or lower.
* The confidence interval shows a range where the "true accuracy" likely is. For example, "between 88% and 92% with 95% confidence."
* The **confidence level (e.g., 95%)** measures how sure you are that the true accuracy lies within this interval. A higher confidence level (like 99%) means you're more certain, but requires a bigger sample.

**Common Z-values:**

| Confidence Level | Z-value |
| ---------------- | ------- |
| 90%              | 1.64    |
| 95%              | 1.96 ✅  |
| 99%              | 2.58    |

The number **1.96** comes from standard statistics (it represents the cutoff points for 95% of all possible outcomes in a normal distribution).

**Practical Example:**

* Suppose you measure a sample accuracy of **93%**.
* With **Z = 1.96** (95% confidence), you might say:

  > "I'm 95% sure the actual accuracy in the full dataset is between **91% and 95%**."

That’s why we pick Z = 1.96 for our initial sampling.

---

## What does p mean? (Estimated Accuracy)

**p** is your initial guess at what the accuracy might be before actually measuring.

Why guess at all?

* When calculating how big your sample should be, you must assume something about the accuracy you're going to measure.
* The value **p = 0.5** (50%) is the most conservative guess. Why conservative? Because if the accuracy is around 50%, the uncertainty (variance) is highest, requiring a larger sample to achieve the desired precision.
* If you already know your accuracy will probably be around **90%**, you can choose **p = 0.9**, which significantly reduces your sample size.

**Example:**

* With **p = 0.5**, Z = 1.96 (95% confidence), and ±2% error margin:

  * You need about **2400** samples.
* If you know your accuracy is closer to **90% (p=0.9)**, the required sample size goes down to about **865** records.

In other words, the better you can estimate your accuracy beforehand, the fewer records you have to sample.

---

## What does E mean? (Margin of Error)

The **margin of error (E)** is the precision of your measurement.

* If your accuracy is **90%** with an error margin of **±2%**, you’re confident the true accuracy is between **88% and 92%**.
* A smaller margin of error (e.g., ±1%) gives you a tighter estimate but requires more data points.
* A bigger margin (e.g., ±5%) requires fewer samples but your estimate is less precise.

---

## How are Z, p, and E connected?

The formula you're using is:

$$
n = \frac{Z^2 \times p \times (1 - p)}{E^2}
$$

* **n**: number of samples you need
* **Z**: determines how sure you want to be (confidence level)
* **p**: your expected accuracy (initial guess)
* **E**: how precise you want your estimate to be (margin of error)

**Example calculation (initial worst-case scenario)**:

* **Confidence:** 95% (**Z = 1.96**)
* **Estimated accuracy:** 50% (**p = 0.5**, conservative assumption)
* **Margin of error:** ±2% (**E = 0.02**)

$$
n = \frac{1.96^2 \times 0.5 \times (1 - 0.5)}{0.02^2} \approx 2400
$$

Thus, you need to review around **2400** records for reliable estimation.

---

## Why start conservatively?

Initially assuming accuracy is **50% (p=0.5)** ensures you don’t underestimate the needed sample size. Once you complete the first assessment and know your accuracy (e.g., 90%), you update **p** to **0.9** in the formula, reducing sample size in future assessments.

---

## Summary

* **Z-value (Confidence)**:
  *How sure do I want to be in my accuracy estimate?* (Commonly 95% → Z=1.96)
* **p (Estimated Accuracy)**:
  *What's my best initial guess of the system's accuracy?* (Start with 0.5, update after first round)
* **E (Margin of Error)**:
  *How precise should my accuracy estimate be?* (Smaller E → larger sample required)

---

**In practice:**

* You first choose a confidence level (usually 95%) and margin of error (±2%).
* Start with a conservative p=0.5 for your first sample calculation.
* Once you've done your first sampling and know the real accuracy, plug that number back into the calculation next time to reduce the sample size.

That’s how these variables directly guide the sampling process, connecting your choice of parameters to the practical size of the dataset you'll need to manually review.



===========================================

# Clarification #2: 

## Why Confidence and Precision Matter?

You have \~200 million records. Checking each one manually is impossible, so you pick a smaller **sample** to check manually.

After checking this sample, you might find something like:

> **"Our sample shows 93% accuracy."**

But how certain can you be that this 93% accuracy truly reflects the accuracy in the entire 200-million-record dataset?

There are two key questions:

1. **Confidence**: How sure are we that our estimate (e.g., 93%) is close to reality?
2. **Precision (Margin of Error)**: How close is “close”? Within ±1%, ±2%, ±5%?

These two ideas determine how many records you need to sample.

---

## Example using our scenario

You're mapping textual lab test results (e.g., "Blood glucose") to standard numerical codes (e.g., LOINC). You manually sample and review some records to measure accuracy.

### Example 1 (95% confidence, ±2% error):

* Suppose you check **2,400 records**.
* You find that **93%** of these sampled records were mapped correctly.

Because you chose a **95% confidence level** and **±2% error margin**, you can confidently say:

> **"We're 95% sure that the actual accuracy across all 200 million records is between 91% and 95%."**

### Example 2 (90% confidence, ±5% error):

* Suppose instead, you check only **270 records**.
* You again find **93% accuracy**.

Because your sample is smaller, your results are less precise. You might now say:

> **"We're 90% sure the actual accuracy is between 88% and 98%."**

Your accuracy measurement is now less precise (±5% instead of ±2%), and your confidence is slightly lower (90% instead of 95%). But the smaller sample was faster and cheaper.

### Example 3 (99% confidence, ±1% error):

* Now imagine you want to be very confident and precise. You check **\~16,500 records**.
* Again, accuracy measured is **93%**.

Now you can confidently say:

> **"We're 99% sure actual accuracy is between 92% and 94%."**

Your result is very precise and certain—but the cost and effort to annotate this large sample are significantly higher.

---

## How do these examples help?

* **Confidence (Z-value)** defines how certain you are about your estimate.

  * Higher confidence (99%) means "I'm very sure", but requires more samples.
  * Lower confidence (90%) means fewer samples, but you're less certain.

* **Margin of Error (E)** defines how precise your accuracy estimate is.

  * Smaller margin (±1%) is more precise, but requires more samples.
  * Larger margin (±5%) is less precise, but needs fewer samples.

* **Estimated accuracy (p)**: Initially you assume worst-case (50%) to ensure your sample isn't too small. Later, actual measured accuracy can reduce sample size for future checks.

---

**In short:**

* To say something meaningful like **"Accuracy is between 91% and 95%"**, you must choose appropriate values for confidence and margin of error.
* The combination you choose directly affects the number of records you must review manually.

This is exactly how **confidence and margin of error** connect to your sampling strategy.



===========================================


# Clarification #3: 

How Z-values are derived, explained in simple terms, step-by-step:


## Step 1: Understand the Normal Distribution

The **Z-value** comes from the **standard normal distribution** (also called the "bell curve"). This curve shows how data typically distributes around an average (mean).

* Most data (about 68%) falls within ±1 standard deviation (σ) of the mean.
* About 95% falls within ±2 standard deviations.
* About 99.7% falls within ±3 standard deviations.

When we use Z-values, we're specifying exactly how far from the mean (center) we need to go to capture a certain percentage of the area under this curve.

---

## Step 2: What Does "90% Confidence" Mean?

When we say **90% confidence**, we mean we want to capture the middle **90%** of all possible outcomes around our measured result. The remaining **10%** of uncertainty is split evenly into two tails—5% in the lower tail and 5% in the upper tail.

This looks like this:

```
          90% confidence interval
|<--------- 90% ---------->|
| 5% |                   | 5% |
Lower tail             Upper tail
```

---

## Step 3: Finding the Z-value for 90% Confidence

The Z-value represents the exact number of standard deviations from the mean needed to capture the desired confidence interval.

* For **90% confidence**, we're looking to cover the central 90% of the curve.
* This leaves **5%** of the data in each tail (10% total, split into two tails).

To find the Z-value for 90%, we ask:

> *What Z-value cuts off exactly 5% in the upper tail?*

This Z-value is typically found using a **standard normal distribution table** (called Z-table):

* Look up **0.9500** (because 100% - 5% = 95%, or 0.9500) in the table.
* You'll see that the Z-value corresponding to **0.9500** cumulative area under the curve is approximately **1.645** (rounded to **1.64**).

Thus, **Z = 1.64** means we're exactly **1.64 standard deviations** away from the mean, capturing exactly **90%** of outcomes (with 5% left on each side).

---

## Examples for Common Confidence Levels:

| Confidence Level | Tail (each side) | Area from mean | Z-value    |
| ---------------- | ---------------- | -------------- | ---------- |
| 90%              | 5%               | 45%            | **1.64** ✅ |
| 95%              | 2.5%             | 47.5%          | **1.96**   |
| 99%              | 0.5%             | 49.5%          | **2.58**   |

* **95%** confidence: leaves **2.5%** in each tail. Z-value is approximately **1.96**.
* **99%** confidence: leaves **0.5%** in each tail. Z-value is approximately **2.58**.

---

## Quick recap:

* A **Z-value** isn't arbitrary—it comes directly from how the normal distribution is structured.
* **90% confidence** → leaves 5% in each tail → **Z ≈ 1.64**.
* You can find Z-values precisely from statistical tables or standard statistical software.

That's how we arrive at these seemingly "precomputed" Z-values—they’re just standardized benchmarks based on the mathematics of normal distributions.


===========================================

# Clarification #4

## Sample Size Calculation for Data Quality Assessment

### Overview

To measure mapping accuracy (from textual medical terms to numerical codes such as LOINC) across our large dataset (\~200 million records), we select a smaller representative sample for expert review. The accuracy found in this sample helps estimate the accuracy of the full dataset.

The quality of our sample selection is measured using two key metrics:

* **Confidence Level**: How sure we are about our accuracy estimate (typically 95%).
* **Margin of Error**: How precisely our sample reflects true dataset accuracy (initially ±2%).

### How Confidence Level and Margin of Error Affect Sample Size

A higher confidence level or smaller margin of error requires a larger sample. Conversely, reducing confidence or accepting a larger margin allows a smaller sample but with less precision.

### Sample Size Formula

The standard formula for estimating the required sample size (**n**) for accuracy measurement (proportion) is:

$$
n = \frac{Z^2 \times p \times (1 - p)}{E^2}
$$

Where:

* **n**: Number of records we must review (sample size).
* **Z**: Corresponds to the confidence level.

  * Example: **1.96** for 95% confidence.
  * Explanation: 95% confidence means we are 95% sure the real accuracy falls within our margin of error.
* **p**: Our initial estimate of dataset accuracy.

  * Initially use **0.5 (50%)** to maximize the sample size (conservative approach).
  * After initial measurements, replace with actual accuracy to reduce future sample sizes.
* **E**: Margin of error (precision of accuracy).

  * Example: **0.02** for ±2%.

### Practical Example (Initial Scenario):

Initially assuming worst-case accuracy (**p = 0.5**), with **95% confidence** (**Z = 1.96**) and a margin of error of **±2%** (**E = 0.02**):

$$
n = \frac{(1.96)^2 \times 0.5 \times (1 - 0.5)}{(0.02)^2} \approx 2400
$$

Thus, we initially need approximately **2,400 records** annotated by experts to reliably estimate the accuracy.

### Adjustments for Future Cycles

* After the first assessment, replace the conservative **p = 0.5** with the actual accuracy measured (for example, 0.9 for 90% accuracy). This reduces the required sample size for subsequent reviews.
* If requirements tighten (e.g., smaller margin of error, higher confidence), recalculate accordingly using the formula above.

### Understanding Z-values (Background):

* The **Z-value** represents the distance (in standard deviations) from the mean that captures a certain confidence level in a normal distribution (bell curve).
* For example:

  * **90% confidence → Z ≈ 1.64**
  * **95% confidence → Z ≈ 1.96** (standard industry choice)
  * **99% confidence → Z ≈ 2.58**

Higher Z-values reflect greater certainty but require larger samples.
